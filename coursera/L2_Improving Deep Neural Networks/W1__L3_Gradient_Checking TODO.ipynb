{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "\n",
    "Give me a proof that your backpropagation is actually working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "from tools.activation_function import sigmoid, ReLU, ReLU_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - How it works?\n",
    "\n",
    "### (1) derivative and derivative_approx\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "- $y = \\theta * x$\n",
    "- $grad_{true} = \\theta$\n",
    "- $grad_{approx} = \\theta \\approx$ $\\frac{\\theta(x + \\epsilon) - \\theta(x - \\epsilon)}{2\\epsilon}$\n",
    "\n",
    "\n",
    "### (2) difference\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "\n",
    "- $np.linalg.norm([3, 4])$ = $||[3, 4]∣∣_2$ = $(3^2 + 4^2)^{\\frac{1}{2}} = 5$\n",
    "\n",
    "### (3) checking\n",
    "$difference < epsilon : True$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward (x, theta):\n",
    "    # eg: y = 2x\n",
    "    return np.dot(theta, x)\n",
    "\n",
    "def backward (x, theta):\n",
    "    # eg: y = 2x 导数为2\n",
    "    return theta\n",
    "\n",
    "def example_checking (x, theta, epsilon):\n",
    "    y_plus = forward(x + epsilon, theta)\n",
    "    y_minus = forward(x - epsilon, theta)\n",
    "    return (y_plus - y_minus) / (2 * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 1. 导数 ===== 4\n",
      "===== 1. 导数预估值 ===== 3.9999999978945766\n",
      "===== 2. Difference 计算 =====\n",
      "===== 3. 校验值对比 =====\n",
      "difference 2.6317792396744613e-10\n",
      "The gradient is correct!\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "epsilon = 1e-7\n",
    "\n",
    "grad_true = backward(x, theta)\n",
    "grad_approx = example_checking(x, theta, epsilon)\n",
    "print('===== 1. 导数 =====', grad_true)\n",
    "print('===== 1. 导数预估值 =====', grad_approx)\n",
    "print('===== 2. Difference 计算 =====') \n",
    "diff_up = np.linalg.norm(grad_true - grad_approx)\n",
    "diff_down = np.linalg.norm(grad_true) + np.linalg.norm(grad_approx)\n",
    "difference = diff_up / diff_down\n",
    "print('===== 3. 校验值对比 =====') \n",
    "if (difference < epsilon):\n",
    "    print('difference', difference)\n",
    "    print('The gradient is correct!')\n",
    "else:\n",
    "    print('The gradient is wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep NN\n",
    "\n",
    "参考: W1__L2_Regularization.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = scipy.io.loadmat('data/data.mat')\n",
    "    train_X = data['X'].T # (2, 211) \n",
    "    train_Y = data['y'].T # (1, 211)\n",
    "    return train_X, train_Y\n",
    "\n",
    "train_X, train_Y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation (X, params):\n",
    "    # 3 Layers (X -> A1(relu) -> A2(relu) -> A3(sigmoid))\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    W3 = params['W3']\n",
    "    \n",
    "    b1 = params['b1']\n",
    "    b2 = params['b2']\n",
    "    b3 = params['b3']\n",
    "    \n",
    "    # Layer 1\n",
    "    A1 = ReLU(np.dot(W1.T, X) + b1)\n",
    "    # Layer 2\n",
    "    A2 = ReLU(np.dot(W2.T, A1) + b2)\n",
    "    # Layer 3\n",
    "    A3 = sigmoid(np.dot(W3.T, A2) + b3)\n",
    "\n",
    "    A = {\n",
    "        'A0': X,\n",
    "        'A1': A1,\n",
    "        'A2': A2,\n",
    "        'A3': A3\n",
    "    }\n",
    "\n",
    "    return A\n",
    "\n",
    "def cost_function (A, y):\n",
    "    A3 = A['A3']\n",
    "    m = y.shape[1]\n",
    "    loss = -y * np.log(A3) - (1 - y) * np.log(1 - A3)\n",
    "    J = (1 / m) * np.sum(loss)\n",
    "\n",
    "    return J\n",
    "\n",
    "def backward_propagation (A, y, params):\n",
    "    m = y.shape[1]\n",
    "    # dJ_dZ3 = A3 - y\n",
    "    dJ_dZ3 = A['A3'] - y\n",
    "    dJ_dW3 = (1 / m) * np.dot(A['A2'], dJ_dZ3.T)\n",
    "    dJ_db3 = (1 / m) * np.sum(dJ_dZ3, axis = 1, keepdims = True)\n",
    "    \n",
    "    # dJ_dZ2\n",
    "    dJ_dZ2 = np.dot(params['W3'], dJ_dZ3) * ReLU_derivative(A['A2'])\n",
    "    dJ_dW2 = (1 / m) * np.dot(A['A1'], dJ_dZ2.T)\n",
    "    dJ_db2 = (1 / m) * np.sum(dJ_dZ2, axis = 1, keepdims = True)\n",
    "    \n",
    "    # dJ_dZ1\n",
    "    dJ_dZ1 = np.dot(params['W2'], dJ_dZ2) * ReLU_derivative(A['A1'])\n",
    "    dJ_dW1 = (1 / m) * np.dot(A['A0'], dJ_dZ1.T)\n",
    "    dJ_db1 = (1 / m) * np.sum(dJ_dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\n",
    "        'dJ_dW3': dJ_dW3,\n",
    "        'dJ_db3': dJ_db3,\n",
    "        'dJ_dW2': dJ_dW2,\n",
    "        'dJ_db2': dJ_db2,\n",
    "        'dJ_dW1': dJ_dW1,\n",
    "        'dJ_db1': dJ_db1\n",
    "    }\n",
    "    \n",
    "    return grads\n",
    "    \n",
    "def update_derivatives (params, grads, alpha, L_len):\n",
    "    for l in range(1, L_len):\n",
    "        params['W' + str(l)] -= alpha * grads['dJ_dW' + str(l)]\n",
    "        params['b' + str(l)] -= alpha * grads['dJ_db'+ str(l)]\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 Weights, bias 平铺\n",
    "def dictionary_to_vector(parameters):\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
    "        # flatten parameter\n",
    "        \"\"\"\n",
    "        W1: (2, 20)\n",
    "        b1: (20, 1)\n",
    "        \n",
    "        parameters[key] = [[1, 2], [3, 4]]\n",
    "        new_vector = [[1], [2], [3], [4]]\n",
    "        keys = ['W1', 'W1', 'W1', 'W1']\n",
    "        \"\"\"\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        keys = keys + [key] * new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            \"\"\"\n",
    "              theta = [[1], [2], [3], [4]]\n",
    "              new_vector = [[5], [6]]\n",
    "              \n",
    "              theta = np.concatenate((theta, new_vector), axis=0)\n",
    "              theta = [[1], [2], [3], [4], [5], [6]]\n",
    "            \"\"\"\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    return theta, keys\n",
    "\n",
    "# 将 导数 平铺\n",
    "def gradients_to_vector (gradients):\n",
    "    count = 0\n",
    "    for key in [\"dJ_dW1\", \"dJ_db1\", \"dJ_dW2\", \"dJ_db2\", \"dJ_dW3\", \"dJ_db3\"]:\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:6].reshape((2,3))\n",
    "    parameters[\"b1\"] = theta[6:9].reshape((3,1))\n",
    "    parameters[\"W2\"] = theta[9:15].reshape((3,2))\n",
    "    parameters[\"b2\"] = theta[15:17].reshape((2,1))\n",
    "    parameters[\"W3\"] = theta[17:19].reshape((2,1))\n",
    "    parameters[\"b3\"] = theta[19:20].reshape((1,1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradient_descent_checking (params, grads, epsilon, X, y):\n",
    "    \"\"\"\n",
    "    1. 平铺所有weights and bias\n",
    "    所有参数都放在了params, 格式是object / dictionary\n",
    "    params = {\n",
    "      'W1': W1,\n",
    "      'W2': W2,\n",
    "      'b1': b1,\n",
    "      'b2': b2\n",
    "    }\n",
    "\n",
    "    return [W1, b2, W2, b2, W3, b3, .....]\n",
    "\n",
    "    2. 平铺 所有dJ_dw, dJ_db\n",
    "    3. 初始化 J_plus, J_minus, grad_approx\n",
    "    4. 平铺的w,b 每个都需要计算\n",
    "\n",
    "    \"\"\"\n",
    "    # [1] 平铺所有weights and bias\n",
    "    theta_arr, keys = dictionary_to_vector(params)\n",
    "    # [2] 平铺所有dJ_db, dJ_dw\n",
    "    grad_arr = gradients_to_vector(grads)\n",
    "    # [3] 初始化 J_plus, J_minus, grad_approx\n",
    "    param_num = theta_arr.shape[0]\n",
    "    J_plus, J_minus, grad_approx = np.zeros((param_num, 1)), np.zeros((param_num, 1)),np.zeros((param_num, 1))\n",
    "    \n",
    "    # [4] 计算\n",
    "    for i in range(param_num):\n",
    "        # (y(x + e) - y(x - e)) / 2e\n",
    "        # theta_plus\n",
    "        plus = np.copy(theta_arr)\n",
    "        plus[i][0] += epsilon\n",
    "        plus_matrix = vector_to_dictionary(plus)\n",
    "        \n",
    "        # y(x + e)\n",
    "        A_plus = forward_propagation(X, plus_matrix)\n",
    "        J_plus[i] = cost_function (A_plus, y)\n",
    "        \n",
    "        # theta_minus\n",
    "        minus = np.copy(theta_arr)\n",
    "        minus[i][0] -= epsilon\n",
    "        minus_matrix = vector_to_dictionary(minus)\n",
    "        # y(x - e)\n",
    "        A_minus = forward_propagation(X, minus_matrix)\n",
    "        J_minus[i] = cost_function (A_minus, y)\n",
    "        \n",
    "        # (y(x + e) - y(x - e)) / 2e\n",
    "        grad_approx[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)   \n",
    "    \n",
    "    print('===== grad_arr =====')\n",
    "    print(grad_arr)\n",
    "    print('===== grad_approx =====')\n",
    "    print(grad_approx)\n",
    "    \n",
    "    numerator = np.linalg.norm(grad_arr - grad_approx)\n",
    "    denominator = np.linalg.norm(grad_arr) + np.linalg.norm(grad_approx)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if difference > epsilon:\n",
    "        print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model ():\n",
    "    def __init__(self, X, y, Layers):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.Layers = Layers\n",
    "        self.L_len = len(Layers)\n",
    "        self.m = X.shape[1]\n",
    "        \n",
    "        # hyperparameters:\n",
    "        self.alpha = 0.01\n",
    "        self.iterations = 100000\n",
    "    \n",
    "    def init_parameters_he (self):\n",
    "        parameters = {}\n",
    "        for i in range(1, len(self.Layers)):\n",
    "            # 缩放因子\n",
    "            he = np.sqrt(2. / self.Layers[i - 1])\n",
    "            parameters['W' + str(i)] = np.random.randn(self.Layers[i - 1], self.Layers[i]) * he\n",
    "            parameters['b' + str(i)] = np.zeros(shape = (self.Layers[i], 1))\n",
    "        return parameters\n",
    "\n",
    "    def training (self):\n",
    "        X, y = self.X, self.y\n",
    "        L_len, alpha = self.L_len, self.alpha\n",
    "        \n",
    "        J_arr = []\n",
    "        \n",
    "        # 1. init parameters\n",
    "        params = self.init_parameters_he()\n",
    "\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # 2. Forward propagation\n",
    "            A = forward_propagation(X, params)\n",
    "\n",
    "            # 3. Loss\n",
    "            if i % 100 == 0:\n",
    "                J = cost_function(A, y)\n",
    "                J_arr.append(J)\n",
    "\n",
    "            # 4. backward_propagation\n",
    "            grads = backward_propagation (A, y, params)\n",
    "            \n",
    "            if (i == self.iterations - 1):\n",
    "                print('====== 梯度检查 start =====')\n",
    "                gradient_descent_checking (params, grads, 1e-7, self.X, self.y)\n",
    "                print('====== 梯度检查 end =====')\n",
    "            \n",
    "            # 5. update_derivatives\n",
    "            params = update_derivatives (params, grads, alpha, L_len)\n",
    "        \n",
    "        # return Weights and bias\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 2, 1]\n",
      "====== 梯度检查 start =====\n",
      "===== grad_arr =====\n",
      "[[ 2.13528362e-04]\n",
      " [ 7.16486517e-03]\n",
      " [-4.00088837e-05]\n",
      " [-3.78436897e-04]\n",
      " [ 3.22127618e-03]\n",
      " [-6.99643331e-04]\n",
      " [ 8.53933126e-04]\n",
      " [-1.44541183e-02]\n",
      " [ 3.35052949e-03]\n",
      " [ 4.98240132e-04]\n",
      " [ 1.34667560e-03]\n",
      " [ 1.46905427e-03]\n",
      " [ 4.13274149e-04]\n",
      " [-1.35246270e-04]\n",
      " [-3.00073968e-04]\n",
      " [ 4.51679861e-04]\n",
      " [ 1.90863111e-03]\n",
      " [ 1.04268263e-03]\n",
      " [ 7.15907399e-04]\n",
      " [-1.00224304e-03]]\n",
      "===== grad_approx =====\n",
      "[[  14.98565164]\n",
      " [  -9.08872396]\n",
      " [   0.28550543]\n",
      " [   6.34792615]\n",
      " [ -10.74532429]\n",
      " [  -5.63222072]\n",
      " [-107.6000092 ]\n",
      " [  25.55294413]\n",
      " [ -39.36782381]\n",
      " [ -48.7045468 ]\n",
      " [ -46.02313121]\n",
      " [  -1.57382981]\n",
      " [  -1.3488579 ]\n",
      " [  -8.04818276]\n",
      " [  -7.35960219]\n",
      " [ -41.77030529]\n",
      " [ -37.24774947]\n",
      " [  19.67993073]\n",
      " [   6.10979928]\n",
      " [  15.61177427]]\n",
      "\u001b[93mThere is a mistake in the backward propagation! difference = 0.9999238087187705\u001b[0m\n",
      "====== 梯度检查 end =====\n"
     ]
    }
   ],
   "source": [
    "Layers = [train_X.shape[0], 3, 2, 1]\n",
    "print(Layers)\n",
    "\n",
    "NN_he = NN_Model(train_X, train_Y, Layers)\n",
    "params_he = NN_he.training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
