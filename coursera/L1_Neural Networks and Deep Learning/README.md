
# Course 1: Neural Networks and Deep Learning

```
Week 2 - Logistic Regression with a Neural Network mindset
Week 3 - Planar data classification with one hidden layer
Week 4 - Building your Deep Neural Network: Step by Step
Week 4 - Deep Neural Network for Image Classification: Application
```
***

# Week2: Logistic Regression
## 1.1 Practice
- 2.13__Vectorizing Logistic Regression.ipynb
- 2.13__Vectorizing Logistic Regression.ipynb
- 2.16__python _ numpy.ipynb

## 1.2 逻辑回归
- W2__Logistic Regression demo.ipynb
- W2__Logistic Regression with a Neural Network mindset.ipynb

```
Tags:
logistic回归、损失函数、梯度下降、计算向量化、代价函数等
Layers:
- no hidden layer
- the output layer with one output

Activation Function:
- sigmoid
```
[R: Logistic Regression with a Neural Network mindset](https://github.com/enggen/Deep-Learning-Coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)

***

# Week3: Shallow neural networks
## 2.1 浅层神经网络实现平面数据分类
- W3__Planar data classification with one hidden layer.ipynb
```

Tags: 神经网络、激活函数、梯度下降法、反向传播、随机初始化

Layers:
- one hidden layer with m neurons
- the output layer with one output

Activation Function:
- tanh
- sigmoid(last layer)
```

[R: Planar data classification with one hidden layer](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)

***

# Week4: Deep Neural Network

## 3.1 构建深层神经网络
**TODO: 跑数据有点问题**
- W4__Building your Deep Neural Network_Step by Step.ipynb

```
Tags: 深度神经网络、DNN的前向和反向传播、参数和超参数等
```

[R: Building your Deep Neural Network: Step by Step](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb)


## 3.2 图像分类应用
- W5__2-Layer__Deep Neural Network for Image Classification - Application.ipynb
- W5__N-Layer__Deep Neural Network for Image Classification - Application.ipynb


[R: Deep Neural Network for Image Classification: Application](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Deep%20Neural%20Network%20-%20Application.ipynb)

***

# 疑问
```html
疑问:

1. 为什么增加层数, 会陷入局部最优, 梯度不会继续下降, 一直固定在0.65左右?

2. 调整parameters 依据? 有点碰运气的感觉?
```