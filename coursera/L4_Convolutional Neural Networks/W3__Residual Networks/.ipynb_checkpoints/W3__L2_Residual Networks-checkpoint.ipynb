{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - The problem of very deep neural networks\n",
    "\n",
    "随着深度越深，vanishing gradients 梯度消失的情况约明显。\n",
    "\n",
    "* The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output). \n",
    "\n",
    "* However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow. \n",
    "\n",
    "* More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode\" to take very large values).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Building a Residual Network\n",
    "$Z1 -> Relu(Z1) -> Maxpool(A1) -> Conv(A1) -> Z2 -> Relu(A1 + Z2) -> ...$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Initializing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset ():\n",
    "    train_data = h5py.File('datasets/train_happy.h5', 'r')\n",
    "    # train_data = { train_set_x, train_set_y }\n",
    "    \n",
    "    train_X = np.array(train_data['train_set_x'][:]) # (600, 64, 64, 3)\n",
    "    train_y = np.array(train_data['train_set_y'][:]) # (600, 1)\n",
    "    \n",
    "    test_data = h5py.File('datasets/test_happy.h5', 'r')\n",
    "    test_X = np.array(test_data['test_set_x'][:]) # (150, 64, 64, 3)\n",
    "    test_y = np.array(test_data['test_set_y'][:]) # (150, 1)\n",
    "    \n",
    "    # the list of classes\n",
    "    \"\"\"\n",
    "    test_data['list_classes'] 返回 <HDF5 dataset \"list_classes\": shape (2,), type \"<i8\">\n",
    "    test_data[\"list_classes\"][:] 返回 [0, 1]\n",
    "    \"\"\"\n",
    "    classes = np.array(test_data[\"list_classes\"][:]) #[0, 1]\n",
    "    train_y = train_y.reshape((1, train_y.shape[0])) # (600, 1) => (1, 600)\n",
    "    test_y = test_y.reshape((1, test_y.shape[0])) # (150, 1) => (1, 150)\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = load_dataset()\n",
    "\n",
    "# Normailize image data\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0\n",
    "\n",
    "# shape y\n",
    "train_y = train_y.T\n",
    "test_y = test_y.T\n",
    "\n",
    "# print(train_X.shape) # (600, 64, 64, 3)\n",
    "# print(train_y.shape) # (600, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Identity block\n",
    "\n",
    "Identity block = 残差模块\n",
    "\n",
    "![image](https://wx3.sinaimg.cn/mw1024/701c57e5ly1ge9afo75lij21eu0eqjue.jpg)\n",
    "\n",
    "\n",
    "**这里我们用3个深度块儿为一组计算**\n",
    "1. $X => Conv2d => BN => RELU => A1$\n",
    "2. $A1 => Conv2d => BN => RELU => A2$\n",
    "3. $A2 => Conv2d => BN => RELU(Z2 + X) => A3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block (X, f, filters_num, key):\n",
    "    \"\"\"\n",
    "    X = (m, n_H, n_W, n_C)\n",
    "    filters_num = { f1, f2, f3 } in each layer for output name\n",
    "    key = key in ResNet\n",
    "    \"\"\"\n",
    "    \n",
    "    # define name of block\n",
    "    conv_name_base = 'CONV' + str(key)\n",
    "    bn_name_base = 'BN' + str(key)\n",
    "    \n",
    "    # filters_num\n",
    "    n_C_1, n_C_2, n_C_3 = filters_num\n",
    "    \n",
    "    n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block (X, f, filters_num, key):\n",
    "    \"\"\"\n",
    "    X = (m, n_H, n_W, n_C)\n",
    "    filters_num = { f1, f2, f3 } in each layer for output name\n",
    "    key = key in ResNet\n",
    "    \"\"\"\n",
    "    # 创建Tensor\n",
    "    # Tensor(\"input_1:0\", shape=(?, 4, 4, 6), dtype=float32)\n",
    "    X = Input(shape = X.shape[1:]) # (n_H_prev * n_H_prev * n_C_prev)\n",
    "    \n",
    "    # define name of block\n",
    "    conv_name_base = 'CONV' + str(key)\n",
    "    bn_name_base = 'BN' + str(key)\n",
    "    \n",
    "    # filters_num\n",
    "    n_C_1, n_C_2, n_C_3 = filters_num\n",
    "    \n",
    "    # ========= Layer 1 =========\n",
    "    # 1. CONV (f * f * n_C_prev * f1)\n",
    "    # Z = Conv2D(32, (7, 7), strides = (1, 1), padding = 'valid', name = 'CONV0')(X)\n",
    "    # Tensor(\"CONV12a_4/BiasAdd:0\", shape=(?, 4, 4, 2), dtype=float32)\n",
    "    Z1_conv = Conv2D(\n",
    "        n_C_1, \n",
    "        (1, 1), \n",
    "        padding = 'valid', \n",
    "        kernel_initializer = glorot_uniform(seed=0), \n",
    "        name = conv_name_base + '2a'\n",
    "    )(X)\n",
    "    \n",
    "    # 2. BN\n",
    "    # Tensor(\"BN12a/cond/Merge:0\", shape=(?, 4, 4, 2), dtype=float32)\n",
    "    Z1_bn = BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '2a'\n",
    "    )(Z1_conv)\n",
    "    \n",
    "    # 3. RELU\n",
    "    A1 = Activation('relu')(Z1_bn)\n",
    "    \n",
    "    # ========= Layer 2 =========\n",
    "    # 1. CONV (f * f * n_C_prev * f2)\n",
    "    # Tensor(\"CONV12b/BiasAdd:0\", shape=(?, 4, 4, 4), dtype=float32)\n",
    "    Z2_conv = Conv2D(\n",
    "        filters = n_C_2, \n",
    "        kernel_size = (f, f), \n",
    "        padding = 'same', \n",
    "        kernel_initializer=glorot_uniform(seed=0), # Xavier统一初始化器\n",
    "        name = conv_name_base + '2b'\n",
    "    )(A1)\n",
    "    \n",
    "    # 2. BN\n",
    "    Z2_bn = BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '2b'\n",
    "    )(Z2_conv)\n",
    "    \n",
    "    # 3. RELU\n",
    "    A2 = Activation('relu')(Z2_bn)\n",
    "    \n",
    "    # ========= Layer 3 =========\n",
    "    # 1. CONV (f * f * n_C_prev * f3)\n",
    "    # Tensor(\"CONV12c_1/BiasAdd:0\", shape=(?, 4, 4, 6), dtype=float32)\n",
    "    Z3_conv = Conv2D(\n",
    "        filters = n_C_3, \n",
    "        kernel_size = (1, 1), \n",
    "        padding = 'valid', \n",
    "        kernel_initializer=glorot_uniform(seed=0), # Xavier统一初始化器\n",
    "        name = conv_name_base + '2c'\n",
    "    )(A2)\n",
    "    \n",
    "    # 2. BN\n",
    "    Z3_bn = BatchNormalization(\n",
    "        axis = 3, \n",
    "        name = bn_name_base + '2c'\n",
    "    )(Z3_conv)\n",
    "    \n",
    "    # 3. RELU  Z3_bn 和 X 尺寸一样\n",
    "    Z3 = Z3_bn + X\n",
    "    A3 = Activation('relu')(Z3)\n",
    "\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== A3 ======\n",
      "Tensor(\"activation_19/Relu:0\", shape=(?, 4, 4, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3, 4, 4, 6) # m = 3, n_W, n_H = 4, n_C = 6\n",
    "\n",
    "A3 = identity_block(X, f = 2, filters_num = [2, 4, 6], key = 1)\n",
    "# Tensor(\"activation_16/Relu:0\", shape=(?, 4, 4, 6), dtype=float32)\n",
    "print(A3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
