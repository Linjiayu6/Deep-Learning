
# Neural Networks and Deep Learning

***

### Week2: Logistic Regression
**Practice:**
1. [2.13] Vectorizing Logistic Regression
2. [2.15] Broadcasting in Python.ipynb
3. [2.16] python _ numpy.ipynb

**Assignment:**
1. [W2-self] Logistic Regression demo.ipynb

2. [W2] Logistic Regression with a Neural Network mindset.ipynb
```
Layers:
- no hidden layer
- the output layer with one output

Activation Function:
- sigmoid
```
[R: Logistic Regression with a Neural Network mindset](https://github.com/enggen/Deep-Learning-Coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb)

***

### Week3: Shallow neural networks
**Assignment:**

1. [W3] Planar data classification with one hidden layer.ipynb

```
Layers:
- one hidden layer with m neurons
- the output layer with one output

Activation Function:
- tanh
- sigmoid(last layer)
```
[R: Planar data classification with one hidden layer](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)
