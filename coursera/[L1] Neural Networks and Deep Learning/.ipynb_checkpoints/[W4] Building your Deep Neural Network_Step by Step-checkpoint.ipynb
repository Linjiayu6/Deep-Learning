{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "- more than one layer\n",
    "- activation function: LeRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "from tools.activation_function import sigmoid, sigmoid_derivative, ReLU, ReLU_derivative, tanh, tanh_derivative\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# keep all the random function calls consistent\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Initalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Layers & Parameters\n",
    "\n",
    "1. hidden layers: 2\n",
    "\n",
    "2. initialize weights and bias in each layers\n",
    "例如: [6, 10, 5, 1]: 输入层6 features, 输出1个, 10为hidden_1, 5为hidden_2\n",
    "\n",
    "**一列为一组数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(1, 209)\n"
     ]
    }
   ],
   "source": [
    "# # 测试数据\n",
    "# X = np.random.rand(4, 20)\n",
    "# Y = np.ones(shape = (1, 20))\n",
    "\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "def load_dataset():  \n",
    "    train_dataset = h5py.File('data/train_catvnoncat.h5', \"r\")\n",
    "    # 209 samples, 64 * 64 pixels\n",
    "    train_X = np.array(train_dataset[\"train_set_x\"][:]) # (209, 64, 64, 3) \n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:]) # (209,)\n",
    "  \n",
    "    test_dataset = h5py.File('data/test_catvnoncat.h5', \"r\")  \n",
    "    # 50 samples\n",
    "    test_X = np.array(test_dataset[\"test_set_x\"][:]) # (50, 64, 64, 3)\n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:]) # (50,)\n",
    "  \n",
    "    # label\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # [b'non-cat' b'cat'] \n",
    "    \n",
    "    # y = [...] => y = [[...]]\n",
    "    train_y = np.array([train_y]) # train_y = train_y.reshape((1, train_y.shape[0]))\n",
    "    test_y = np.array([test_y]) # test_y = test_y.reshape((1, test_y.shape[0])) \n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, classes\n",
    "\n",
    "train_X, train_y, test_X, test_y, classes = load_dataset()\n",
    "\n",
    "X = train_X.reshape(train_X.shape[0], -1).T \n",
    "X = X / 255\n",
    "Y = train_y\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_list = [3, 2, 1]\n",
    "layer_list = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers (X, layer_list):\n",
    "    L_len = len(layer_list)\n",
    "    \n",
    "    parameters = {\n",
    "        'n_input': X.shape[0] # layer0 - features_number\n",
    "    }\n",
    "\n",
    "    for i in range(L_len):\n",
    "        num = layer_list[i]\n",
    "        if i == L_len - 1:\n",
    "            parameters['n_ouput' + str(i + 1)] = num\n",
    "        else:\n",
    "            parameters['n_hidden_' + str(i + 1)] = num\n",
    "    return parameters\n",
    "\n",
    "layer_parameters_obj = init_layers(X, layer_list)\n",
    "# eg: {'n_input': 3, 'n_hidden_1': 5, 'n_hidden_2': 4, 'n_ouput3': 1}\n",
    "\n",
    "layer_parameters_values = list(layer_parameters_obj.values())\n",
    "# eg: [3, 5, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重和偏置: {'W1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'b1': array([[0.]])}\n",
      "W1 (12288, 1)\n",
      "b1 (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def init_parameters (layer_parameters_values):\n",
    "    N_Layers = layer_parameters_values\n",
    "    # [3, 5, 4, 1]: 输入层3 features, 输出1个, 5为hidden_1, 4为hidden_2\n",
    "    parameters = {}\n",
    "    \n",
    "    # Weights and bias\n",
    "    for i in range(len(N_Layers) - 1):\n",
    "        n_prev = N_Layers[i]\n",
    "        n_next = N_Layers[i + 1]\n",
    "        # 一列为一组数据\n",
    "        # parameters['W' + str(i + 1)] = np.random.rand(n_prev, n_next)\n",
    "        parameters['W' + str(i + 1)] = np.zeros(shape = (n_prev, n_next))\n",
    "        parameters['b' + str(i + 1)] = np.zeros(shape = (n_next, 1)) \n",
    "    return parameters\n",
    "\n",
    "weights_bias_parameters = init_parameters (layer_parameters_values)\n",
    "\n",
    "print('权重和偏置:', weights_bias_parameters)\n",
    "\n",
    "print('W1', weights_bias_parameters['W1'].shape)\n",
    "print('b1', weights_bias_parameters['b1'].shape)\n",
    "\n",
    "# print('W2', weights_bias_parameters['W2'].shape)\n",
    "# print('b2', weights_bias_parameters['b2'].shape)\n",
    "\n",
    "# print('W3', weights_bias_parameters['W3'].shape)\n",
    "# print('b3', weights_bias_parameters['b3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Forward Propagation\n",
    "\n",
    "$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$\n",
    "\n",
    "$A = \\sigma(Z)$\n",
    "\n",
    "\n",
    "```\n",
    "X: 3 * 20\n",
    "W1 = 3 * 5  b2 = 5 * 1\n",
    "\n",
    "Z1/A1 = 5 * 20\n",
    "Z1 = np.dot(W1.T, X) + b2\n",
    "A1 = ReLU(Z1)\n",
    "\n",
    "最后\n",
    "A3 = sigmoid(Z3)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A0': array([[0.06666667, 0.76862745, 0.32156863, ..., 0.56078431, 0.08627451,\n",
      "        0.03137255],\n",
      "       [0.12156863, 0.75294118, 0.27843137, ..., 0.60784314, 0.09411765,\n",
      "        0.10980392],\n",
      "       [0.21960784, 0.74509804, 0.26666667, ..., 0.64705882, 0.09019608,\n",
      "        0.20784314],\n",
      "       ...,\n",
      "       [0.        , 0.32156863, 0.54117647, ..., 0.33333333, 0.01568627,\n",
      "        0.        ],\n",
      "       [0.        , 0.31372549, 0.55294118, ..., 0.41960784, 0.01960784,\n",
      "        0.        ],\n",
      "       [0.        , 0.31764706, 0.55686275, ..., 0.58431373, 0.        ,\n",
      "        0.        ]]), 'Z1': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.]]), 'A1': array([[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
      "        0.5]])}\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation (X, layer_parameters_values, weights_bias_parameters):\n",
    "    \"\"\"\n",
    "    - X 输入; \n",
    "    - layer_parameters_values 每层的节点个数\n",
    "    - weights_bias_parameters: 从第1层开始的weights 和 bias\n",
    "    \"\"\"\n",
    "    parameters = { \"A0\": X }\n",
    "    L_num = len(layer_parameters_values)\n",
    "    \n",
    "    \"\"\"\n",
    "    W1 = weights_bias_parameters['W1']\n",
    "    b1 = weights_bias_parameters['b1']\n",
    "    \n",
    "    output_Z = np.dot(W1.T, input_A)\n",
    "    output_A = ReLU(output_Z)\n",
    "    \"\"\"\n",
    "    for l in range(1, L_num):\n",
    "        W = weights_bias_parameters['W' + str(l)]\n",
    "        b = weights_bias_parameters['b' + str(l)]\n",
    "        A_prev = parameters['A' + str(l - 1)]\n",
    "        \n",
    "        output_Z = np.dot(W.T, A_prev) + b\n",
    "        parameters['Z' + str(l)] = output_Z\n",
    "\n",
    "        \n",
    "        # 最后一个结点是否为1个, 如果为1个, 那么最后一个activation function 为 sigmoid\n",
    "        if l == L_num - 1 and layer_parameters_values[-1] == 1:\n",
    "            output_A = sigmoid(output_Z)\n",
    "            parameters['A' + str(l)] = output_A\n",
    "        else:\n",
    "            # output_A = ReLU(output_Z)\n",
    "            output_A = np.tanh(output_Z)\n",
    "            parameters['A' + str(l)] = output_A\n",
    "\n",
    "    return parameters\n",
    "    \n",
    "A_parameters = forward_propagation(X, layer_parameters_values, weights_bias_parameters)\n",
    "\n",
    "print(A_parameters)\n",
    "\n",
    "# print('A0:', A_parameters['A0'])\n",
    "# print('A1:', A_parameters['A1'])\n",
    "# print('A2:', A_parameters['A2'])\n",
    "# print('A3:', A_parameters['A3'])\n",
    "\n",
    "# print('A0.shape:', A_parameters['A0'].shape)\n",
    "# print('A1.shape:', A_parameters['A1'].shape)\n",
    "# print('A2.shape:', A_parameters['A2'].shape)\n",
    "# print('A3.shape:', A_parameters['A3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 - Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function (A, Y):\n",
    "    m = A.shape[1]\n",
    "    epsilon = 1e-5\n",
    "    J = (1 / m) * np.sum(-Y * np.log(A + epsilon) - (1 - Y) * np.log(1 - A + epsilon))\n",
    "\n",
    "    # it turns [[ 10 ]] to 10\n",
    "    return np.squeeze(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Backward Propagation\n",
    "\n",
    "### 1. J_W3, dJ_b3\n",
    "\n",
    "#### 1.1 dJ_dA3\n",
    "- dJ_dA3 = -Y / A3 + (1 - Y) / (1 - A3)\n",
    "\n",
    "#### 1.2 dJ_dZ3\n",
    "- dJ_dZ3 = dJ_dA3 * sigmoid_derivative(A3) = A3 - Y\n",
    "\n",
    "if sigmoid:\n",
    "- sigmoid_derivative(A3) = A3 * (1 - A3)\n",
    "- dJ_dZ3 = dJ_dA3 * sigmoid_derivative(A3) = A3 - Y\n",
    "- (或1.2) dJ_dZ3 = dJ_dA3 * ReLU_derivative(A3)\n",
    " \n",
    "if ReLU:\n",
    "- dJ_dZ3 = dJ_dA3 * ReLU_derivative(A3)\n",
    "\n",
    "#### 1.3 dJ_dW3 / dJ_db3\n",
    "- 上一层A值 * 本层的差值.T\n",
    "- dJ_dW3 = (1 / m) * np.dot(A2, dJ_dZ3.T)\n",
    "- dJ_db3 = (1 / m) * np.sum(dJ_dZ3, axis = 1, keepdims = True)\n",
    "\n",
    "### 2. dJ_W2, dJ_b2\n",
    "\n",
    "- dZ3_dA2 = W3\n",
    "- dA2_dZ2 = ReLU_derivative(A2)\n",
    "\n",
    "#### 2.1 dJ_dZ2\n",
    "- dJ_dZ2 = dJ_dZ3(上层到Z差值) * dZ3_dA2(上一层权重) * dA2_dZ2(这层导数)\n",
    "\n",
    "#### 2.2 dJ_W2 / dJ_b2\n",
    "- dJ_W2 = 1 / m * (A1 * dJ_dZ2.T)\n",
    "- dJ_b2 =  1 / m * np.sum(dJ_dZ2, axis = 1, keepdims = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_last_layer (A_parameters, layer_parameters_values):\n",
    "    L_num = len(layer_parameters_values)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # 最后一个 A, 倒数第二个 A_prev\n",
    "    A = A_parameters['A' + str(L_num - 1)]\n",
    "    A_prev = A_parameters['A' + str(L_num - 2)]\n",
    "    \n",
    "    dJ_dZ_last = []\n",
    "    if (layer_parameters_values[-1] == 1):\n",
    "        # sigmoid: output_layer 结点数为1\n",
    "        dJ_dZ_last = A - Y\n",
    "    else: \n",
    "        # ReLu:\n",
    "        dJ_dA_last = -Y / A + (1 - Y) / (1 - A)\n",
    "        # dJ_dZ_last = dJ_dA_last * ReLU_derivative(A)\n",
    "        J_dZ_last = dJ_dA_last * tanh_derivative(A)\n",
    "\n",
    "    diff = dJ_dZ_last\n",
    "\n",
    "    dJ_dW_last = (1 / m) * (np.dot(A_prev, diff.T))\n",
    "    dJ_db_last = (1 / m) * np.sum(diff, axis = 1, keepdims = True)\n",
    "    \n",
    "    return dJ_dW_last, dJ_db_last, dJ_dZ_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation (weights_bias_parameters, layer_parameters_values, A_parameters, Y):\n",
    "    L = len(layer_parameters_values) # 层数\n",
    "    m = Y.shape[1] # 样本个数\n",
    "    \n",
    "    J = cost_function(A_parameters['A' + str(L - 1)], Y) # 计算代价函数值\n",
    "    \n",
    "    # L - 1: 最后一层特殊处理\n",
    "    dJ_dW_last, dJ_db_last, dJ_dZ_last = backward_last_layer(A_parameters, layer_parameters_values)\n",
    "    derivatives_parameters = {\n",
    "        \"dJ_dW\" + str(L - 1): dJ_dW_last,\n",
    "        \"dJ_db\" + str(L - 1): dJ_db_last,\n",
    "        \"dJ_dZ\" + str(L - 1): dJ_dZ_last\n",
    "    }\n",
    "\n",
    "    # L = 4, L - 1最后一层已经判断完成, 需要倒着从 L - 2开始 (eg: 2 -> 1)\n",
    "    for i in reversed(range(1, L - 1)): # range(1, 3), 只会输出1, 2\n",
    "        # 1. 上一层权重 dZ3_dA2\n",
    "        W_next = weights_bias_parameters['W' + str(i + 1)] # 4 * 1\n",
    "        A_current = A_parameters['A' + str(i)] # 4 * 20\n",
    "        A_prev = A_parameters['A' + str(i - 1)] # 5 * 20\n",
    "        \n",
    "        # 2. 上一层差值 dJ_dZ3\n",
    "        diff_next = derivatives_parameters['dJ_dZ' + str(i + 1)] # 1 * 20\n",
    "        \n",
    "        # 3. 这层导数 dA2_dZ2\n",
    "        # relu_derivative_current = ReLU_derivative(A_current) # 4 * 20\n",
    "        tanh_derivative_current = tanh_derivative(A_current)\n",
    "        \n",
    "        # 4. 该层差值 dJ_dZ = np.dot(上一层weights, 上一层差值) * 当前层导数\n",
    "        # print('上一层weights:', W_next.shape) # (2, 1)\n",
    "        # print('上一层差值:', diff_next.shape) # (1, 20)\n",
    "        # print('当前层导数:', relu_derivative_current.shape) # 2 * 20\n",
    "\n",
    "        # dJ_dZ = np.dot(W_next, diff_next) * relu_derivative_current\n",
    "        dJ_dZ = np.dot(W_next, diff_next) * tanh_derivative_current\n",
    "        \n",
    "        diff = dJ_dZ\n",
    "\n",
    "        derivatives_parameters['dJ_dZ' + str(i)] = dJ_dZ\n",
    "        derivatives_parameters['dJ_dW' + str(i)] = (1 / m) * np.dot(A_prev, diff.T)\n",
    "        derivatives_parameters['dJ_db' + str(i)] = (1 / m) * np.sum(diff, axis = 1, keepdims = True)\n",
    "    \n",
    "    print(derivatives_parameters)\n",
    "    return derivatives_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dJ_dW1': array([[0.04720893],\n",
      "       [0.06299841],\n",
      "       [0.04923539],\n",
      "       ...,\n",
      "       [0.05074585],\n",
      "       [0.0621259 ],\n",
      "       [0.03245145]]), 'dJ_db1': array([[0.15550239]]), 'dJ_dZ1': array([[ 0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,\n",
      "        -0.5,  0.5, -0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,\n",
      "         0.5,  0.5, -0.5, -0.5,  0.5, -0.5,  0.5, -0.5,  0.5,  0.5,  0.5,\n",
      "         0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5, -0.5,  0.5,\n",
      "         0.5,  0.5,  0.5, -0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5, -0.5,\n",
      "         0.5, -0.5, -0.5,  0.5, -0.5, -0.5, -0.5,  0.5,  0.5,  0.5,  0.5,\n",
      "         0.5,  0.5, -0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5,  0.5,\n",
      "         0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -0.5, -0.5,  0.5,  0.5,  0.5,\n",
      "        -0.5,  0.5,  0.5,  0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5,  0.5,\n",
      "         0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5, -0.5, -0.5,\n",
      "        -0.5, -0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,\n",
      "        -0.5,  0.5,  0.5, -0.5,  0.5, -0.5,  0.5, -0.5, -0.5,  0.5,  0.5,\n",
      "         0.5, -0.5, -0.5, -0.5, -0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,\n",
      "         0.5, -0.5, -0.5, -0.5,  0.5, -0.5, -0.5,  0.5,  0.5,  0.5, -0.5,\n",
      "         0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5, -0.5,\n",
      "         0.5, -0.5,  0.5,  0.5, -0.5, -0.5, -0.5,  0.5,  0.5, -0.5, -0.5,\n",
      "         0.5, -0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5,  0.5, -0.5,  0.5,\n",
      "         0.5, -0.5,  0.5,  0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5, -0.5,\n",
      "         0.5,  0.5, -0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]])}\n"
     ]
    }
   ],
   "source": [
    "derivatives_parameters = backward_propagation (weights_bias_parameters, layer_parameters_values, A_parameters, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (weights_bias_parameters, derivatives_parameters, layer_parameters_values):\n",
    "    # hyperparameters\n",
    "    alpha = 0.009\n",
    "    interation = 1000\n",
    "    \n",
    "    # parameters\n",
    "    L = len(layer_parameters_values)\n",
    "    J_arr = []\n",
    "    \n",
    "    for loop_index in range(interation):\n",
    "        for layer in reversed(range(1, L)): # 1到L-1\n",
    "            weights_bias_parameters['W' + str(layer)] -= alpha * derivatives_parameters['dJ_dW' + str(layer)]\n",
    "            weights_bias_parameters['b' + str(layer)] -= alpha * derivatives_parameters['dJ_db' + str(layer)]\n",
    "            \n",
    "    return weights_bias_parameters\n",
    "\n",
    "weights_bias_parameters = train (weights_bias_parameters, derivatives_parameters, layer_parameters_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_parameters {'A0': array([[0.06666667, 0.76862745, 0.32156863, ..., 0.56078431, 0.08627451,\n",
      "        0.03137255],\n",
      "       [0.12156863, 0.75294118, 0.27843137, ..., 0.60784314, 0.09411765,\n",
      "        0.10980392],\n",
      "       [0.21960784, 0.74509804, 0.26666667, ..., 0.64705882, 0.09019608,\n",
      "        0.20784314],\n",
      "       ...,\n",
      "       [0.        , 0.32156863, 0.54117647, ..., 0.33333333, 0.01568627,\n",
      "        0.        ],\n",
      "       [0.        , 0.31372549, 0.55294118, ..., 0.41960784, 0.01960784,\n",
      "        0.        ],\n",
      "       [0.        , 0.31764706, 0.55686275, ..., 0.58431373, 0.        ,\n",
      "        0.        ]]), 'Z1': array([[-1777.168179  , -2900.50957997, -3382.48639095, -1692.26063708,\n",
      "        -1748.41521167, -1560.32653764, -1734.84231751, -2045.4801    ,\n",
      "        -2546.22290989, -2219.02396152, -3459.47141239, -2815.97309482,\n",
      "        -2156.22832172, -3656.47717091, -3219.7034165 , -3839.48236287,\n",
      "        -3540.2581838 , -4273.44745683, -3140.14223738, -1439.73834473,\n",
      "        -2973.84408205, -1860.47790864, -4357.1453711 , -3946.92860284,\n",
      "        -3328.42558434, -1071.95413354, -2631.08153209, -2829.3070969 ,\n",
      "        -2052.96757802, -2488.3936117 , -1420.32102449, -3239.03721991,\n",
      "        -2817.58295194, -3136.06328538, -2136.70289631, -1940.12665784,\n",
      "        -2476.12002748, -4094.40491217,  -573.51697753, -2447.78374795,\n",
      "        -2562.67997583, -1162.68420821, -2966.16198705, -3322.63017334,\n",
      "        -2741.40071919, -1677.65669409, -2837.12081124, -2162.36236552,\n",
      "        -2090.19580139, -2476.99516233, -3952.10048377, -2965.98236056,\n",
      "        -3084.86607275, -3950.17562093, -2088.69930266, -3808.29302627,\n",
      "        -2352.17958875, -4430.63940465, -1224.98764226, -1843.95295144,\n",
      "        -1939.08449926, -2584.02253473, -3306.93471631, -2354.86272313,\n",
      "        -4309.57517624, -1875.87076969, -3435.59170908, -2611.49330243,\n",
      "        -3138.1909624 , -3381.82813463, -3049.89252993, -2125.31600603,\n",
      "        -1402.70660088, -2114.88401285, -2783.68501912, -3265.13827983,\n",
      "        -2257.00576745, -2281.65199119, -2146.1069777 , -1053.54419165,\n",
      "        -2718.13180146, -3182.47008129, -1515.39853413, -1658.11720336,\n",
      "        -2974.87636231, -2849.08266684, -3310.03796493, -1899.93999238,\n",
      "        -2899.29955597, -1566.60892767, -3186.95414546, -2548.32038774,\n",
      "        -2303.62051059, -2482.99329415, -2976.15416566, -3982.65172298,\n",
      "        -3663.90765219, -2378.39979305, -3918.05205477, -3121.9489982 ,\n",
      "        -3162.43506035, -2505.77269019, -3185.54400523, -2526.31058824,\n",
      "        -2079.13044155, -3587.39287528, -3433.62019933, -2598.79537425,\n",
      "        -2508.94495488, -3234.40511912, -3226.77677356, -2613.12976937,\n",
      "        -1645.3202447 , -3217.74360193, -3657.91721395, -2881.30408967,\n",
      "        -2722.26847006, -2375.08164467, -1977.75750103, -2880.40440622,\n",
      "        -2606.67104485, -3798.98480058, -2603.81604212, -3124.63102763,\n",
      "        -2814.50352511, -2425.75673019, -2663.36150759, -2030.46226089,\n",
      "        -3542.83989106, -1522.40550819, -2414.95355938, -3077.57236238,\n",
      "        -2314.40824986, -1952.97858148, -2167.18498171, -3776.12873628,\n",
      "        -3673.91541961, -3682.93042003, -2841.57012003, -2068.58263009,\n",
      "        -2921.60848165, -3117.80050893, -2753.15805069, -1361.89486167,\n",
      "        -3788.54299829, -2551.83417725, -3563.48314531, -3204.42389133,\n",
      "        -1273.97776626, -3194.57383619, -3823.91041605, -1704.64560554,\n",
      "        -2736.49481101, -3037.52403503, -1725.92947931, -2917.45388686,\n",
      "        -2873.96262645, -1903.65995927, -3513.67968411, -2490.03416698,\n",
      "        -2141.66197348, -3059.72614559, -3305.65837287, -2563.42921342,\n",
      "        -3645.78773232, -1852.91816328, -3304.31164616, -2811.89497856,\n",
      "        -2383.17044817, -3505.1671118 , -3119.61613814, -2639.93121074,\n",
      "        -3524.44287181, -3340.18129336, -3173.23254946, -2116.8164143 ,\n",
      "        -2728.3058807 , -2571.35593848, -2284.30360093, -2590.38178043,\n",
      "        -2319.10757305, -3678.67633582, -2813.69212397, -3424.40923528,\n",
      "        -2534.10482343, -2832.19558153, -2127.68287247, -2819.91482095,\n",
      "        -3173.36931574, -2735.12802868, -3789.35485538, -3739.06249532,\n",
      "        -2817.98534859, -4243.95020745, -2302.3117998 ,  -280.17593252,\n",
      "        -2818.69864141, -2652.17261039, -2254.97298952, -2247.70338471,\n",
      "        -3099.42631778, -3377.90406285, -2780.64326253, -2591.81106041,\n",
      "        -4193.89894472,  -677.77707025, -2995.47506167,  -610.98701876,\n",
      "        -1630.65328653]]), 'A1': array([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 8.40894243e-250,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        2.09478061e-122, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
      "        0.00000000e+000, 4.41731562e-295, 0.00000000e+000,\n",
      "        4.48445054e-266, 0.00000000e+000]])}\n",
      "最后的值: [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 8.40894243e-250 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 2.09478061e-122\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 4.41731562e-295 0.00000000e+000 4.48445054e-266\n",
      "  0.00000000e+000]]\n",
      "最后的值处理: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "代价函数值:  3.9661687248072073\n",
      "accuracy: % 65.55023923444976\n"
     ]
    }
   ],
   "source": [
    "def predict (X, layer_parameters_values, weights_bias_parameters, Y):\n",
    "    L = len(layer_parameters_values)\n",
    "    A_parameters = forward_propagation (X, layer_parameters_values, weights_bias_parameters)\n",
    "    \n",
    "    A_last = A_parameters['A' + str(L - 1)]\n",
    "    print('A_parameters', A_parameters)\n",
    "    print('最后的值:', A_last)\n",
    "    \n",
    "    m = A_last.shape[1]\n",
    "    Y_predict = np.zeros((1, m))\n",
    "    for i in range(m):\n",
    "        Y_predict[0, i] =  1 if A_last[0, i] > 0.5 else 0\n",
    "    print('最后的值处理:', Y_predict)\n",
    "\n",
    "    J_A_last = cost_function(A_last, Y)\n",
    "    print('代价函数值: ', J_A_last)\n",
    "    \n",
    "    accuracy = (1 - np.mean(abs(Y_predict - Y))) * 100\n",
    "    print('accuracy: %', format(accuracy))\n",
    "\n",
    "predict (X, layer_parameters_values, weights_bias_parameters, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
