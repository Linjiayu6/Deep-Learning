{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Recurrent Neural Network - Step by Step\n",
    "\n",
    "\n",
    "- $[l]$ 第l层\n",
    "- $(i)$ 第i个例子/样本\n",
    "- <$t$> 第t个time-step\n",
    "- $a_5^{(2)[3]<4>}$ 第三层 第二个样本 第4time-step 第5个输入a值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x = [1, 2, 3]\n",
    "    t = [e^1, e^2, e^3]\n",
    "    sum(t) = e^1 + e^2 + e^3 = _sum\n",
    "    t = t / sum(t) = t / _sum = [e^1/_sum, e^2/_sum, e^3/_sum]\n",
    "    \"\"\"\n",
    "    t = np.exp(x - np.max(x))\n",
    "    return t / t.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward Propagation for basic RNN\n",
    "\n",
    "- $T_x = 10$ time steps 一句话有10个单词\n",
    "- $n_x$ 一个字典里有n个单词, $x^{(i)<t>}$ 第i个样本的第t个timestep shape为(n,)\n",
    "- $m$ mini-batch is m\n",
    "\n",
    "- ($n_x$, m, $T_x$) input x is fed into the RNN\n",
    "\n",
    "![image](https://wx4.sinaimg.cn/mw1024/701c57e5gy1gemde9apnxj211k0eujt1.jpg)\n",
    "![image](https://wx1.sinaimg.cn/mw1024/701c57e5gy1gemde9kvxdj213m0gwtbh.jpg)\n",
    "\n",
    "```python\n",
    "# inputs: a_prev, x\n",
    "# parameters: Waa, Wax, ba, Way, by\n",
    "\n",
    "# 1. compute a_next\n",
    "z = Waa * a_prev + Wax * x + ba\n",
    "a_next = tanh(z)\n",
    "\n",
    "# 2. compute y\n",
    "y = softmax(Way * a_next + by)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    # parameters\n",
    "    Wax = parameters['Wax'] # 5 * 5\n",
    "    Waa = parameters['Waa'] # 5 * 3\n",
    "    ba = parameters['ba'] # 5 * 1\n",
    "\n",
    "    Wya = parameters['Wya'] # 2 * 5\n",
    "    by = parameters['by'] # 2 * 1\n",
    "    \n",
    "    # 1. compute a_next\n",
    "    # 当前状态值 = 前一个状态值Waa * a_prev + 当前输入值Wax * xt + bias\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba) # 5 * 10\n",
    "    \n",
    "    # 2. compute y\n",
    "    # 当前状态值 * weights + bias\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by) # 2 * 10\n",
    "    \n",
    "    # 3. store cache for backpropagation\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] = \n",
      " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape = \n",
      " (5, 10)\n",
      "yt_pred[1] =\n",
      " [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape = \n",
      " (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba'] = np.random.randn(5,1)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
    "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
    "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - RNN forward pass\n",
    "\n",
    "- $a\\langle t-1 \\rangle$: previous cell\n",
    "- $x\\langle t \\rangle$: input data\n",
    "- $a\\langle t \\rangle$: hidden state\n",
    "- $y\\langle t \\rangle$: output prediction\n",
    "- Waa, Wya, Wax, ba, by => weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
